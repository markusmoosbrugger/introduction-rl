{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL Grid Navigation\n",
    "\n",
    "The goal of this assignment is to train an agent to navigate efficiently within a grid.\n",
    "This means the agent should avoid the fields where bombs are placed and find the shortest path\n",
    "from its starting position to the end position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "\n",
    "from grid_env import GridEnv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = GridEnv()\n",
    "\n",
    "# Show action space, observation space and reward range\n",
    "print(f'Action space: {env.action_space}')\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Reward range: {env.reward_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reset environment and print starting position\n",
    "print(f'Starting position: {env.reset()}')\n",
    "\n",
    "# Render the environment\n",
    "env.render(\"image\")\n",
    "\n",
    "# Sample a random action and a random observation\n",
    "print(f'Sample action: {env.action_space.sample()}')\n",
    "print(f'Sample observation: {env.observation_space.sample()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a few steps with random actions\n",
    "env.reset()\n",
    "num_steps = 3\n",
    "for step in range(num_steps):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(f'Action: {env.get_action_name(action)}, observation: {observation}, reward: {reward}, done: {done}.')\n",
    "    env.render(\"image\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(env):\n",
    "    # Q table is dict of dict\n",
    "    Q = {}\n",
    "    for i in range(env.width):\n",
    "        for j in range(env.height):\n",
    "            Q[(i, j)] = {}\n",
    "            for a in env.action_names:\n",
    "                Q[(i, j)][a] = 0\n",
    "    return Q\n",
    "\n",
    "\n",
    "def get_action(env, model, state, epsilon):\n",
    "    random_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "    # exploit\n",
    "    if random_tradeoff > epsilon:\n",
    "        action_dict = model.get(state)\n",
    "        action_name = max(action_dict, key=action_dict.get)\n",
    "\n",
    "        # get index number of action\n",
    "        return env.get_action_index(action_name)\n",
    "\n",
    "    # explore\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "\n",
    "def update_model_sarsa(env, model, state, next_state, reward, action, next_action, alpha, gamma):\n",
    "    pass\n",
    "\n",
    "\n",
    "def update_model_qlearning(env, model, state, next_state, reward, action, alpha, gamma):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_epsilon(episode, min_epsilon=0.01, max_epsilon=1, decay=0.01):\n",
    "    return min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SARSA\n",
    "\n",
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_sarsa(env, episodes=1000):\n",
    "    # variables\n",
    "    total_timesteps = 0\n",
    "    scores = []\n",
    "    gamma = 0.7  # discount factor\n",
    "    alpha = 0.5  # learning rate\n",
    "\n",
    "    model = get_model(env)\n",
    "\n",
    "    # TODO: remove this return statement\n",
    "    return model, scores\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # start episode and get initial observation\n",
    "        state = env.reset()\n",
    "        epsilon = get_epsilon(episode)\n",
    "\n",
    "        # reset score and epochs\n",
    "        score = 0\n",
    "        epochs = 0\n",
    "\n",
    "        # get first action\n",
    "        action = get_action(env, model, state, epsilon)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # loop through steps\n",
    "        while not done:\n",
    "            # TODO implement SARSA\n",
    "\n",
    "            # update score\n",
    "            score += reward\n",
    "\n",
    "            epochs += 1\n",
    "\n",
    "        print(f'Episode {episode} finished, Score: {score}, Epochs: {epochs}, '\n",
    "              f'Epsilon: {epsilon:.2f}')\n",
    "\n",
    "        total_timesteps += epochs\n",
    "        scores.append(score)\n",
    "\n",
    "    # close the environment\n",
    "    env.close()\n",
    "\n",
    "    return model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = GridEnv()\n",
    "episodes = 500\n",
    "model, scores = train_sarsa(env, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_training_development(episodes, scores):\n",
    "    x = range(episodes)\n",
    "    plt.plot(x, scores)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Training scores\")\n",
    "    plt.title(\"Score development over all episodes\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize Q-Table\n",
    "df = pd.DataFrame(model)\n",
    "display(df)\n",
    "\n",
    "# Visualize training development\n",
    "#visualize_training_development(episodes, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization of the q-values\n",
    "In the following grid the learned policy is plotted. Each arrow indicates the action with the highest q-value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rendering import *\n",
    "\n",
    "\n",
    "def visualize_q_values(model):\n",
    "    places = list(model.keys())\n",
    "    renderer = ImageRenderer(env.width, env.height)\n",
    "    renderer.plot_grid()\n",
    "    renderer.plot_rect(env.end_pos, \"green\")\n",
    "\n",
    "    for bomb_pos in env.bomb_positions:\n",
    "        renderer.plot_rect(bomb_pos, \"red\")\n",
    "\n",
    "    for p in places:\n",
    "        if p in env.bomb_positions:\n",
    "            continue\n",
    "\n",
    "        left_q = model[p][\"left\"]\n",
    "        right_q = model[p][\"right\"]\n",
    "        up_q = model[p][\"up\"]\n",
    "        down_q = model[p][\"down\"]\n",
    "\n",
    "        if left_q == max(left_q, right_q, up_q, down_q):\n",
    "            renderer.plot_left(p)\n",
    "\n",
    "        if right_q == max(left_q, right_q, up_q, down_q):\n",
    "            renderer.plot_right(p)\n",
    "\n",
    "        if up_q == max(left_q, right_q, up_q, down_q):\n",
    "            renderer.plot_up(p)\n",
    "\n",
    "        if down_q == max(left_q, right_q, up_q, down_q):\n",
    "            renderer.plot_down(p)\n",
    "\n",
    "\n",
    "visualize_q_values(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_agent(env, model):\n",
    "    # TODO remove this return statement\n",
    "    return 0\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_steps = 0\n",
    "    while not done:\n",
    "        env.render(\"image\")\n",
    "        action = get_action(env, model, state, 0)\n",
    "        print(f'Action: {env.get_action_name(action)}')\n",
    "        state, reward, done, info = env.step(action)\n",
    "        num_steps += 1\n",
    "\n",
    "    print(f'Finished in {num_steps} steps.')\n",
    "    env.render(\"image\")\n",
    "    return num_steps\n",
    "\n",
    "\n",
    "def evaluate_agent(env, num_steps_agent):\n",
    "    num_steps_optimum = env.get_minimum_number_of_steps()\n",
    "    print(f'Minimum number steps required: {num_steps_optimum}')\n",
    "    print(f'Number steps test agent: {num_steps_agent}')\n",
    "\n",
    "    if num_steps_agent == num_steps_optimum:\n",
    "        print(f'SUCCESS! Optimal strategy found by agent.')\n",
    "    else:\n",
    "        print(f'FAILURE! Suboptimal strategy found by agent. '\n",
    "              f'Difference to optimal strategy: {num_steps_agent - num_steps_optimum} steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = test_agent(env, model)\n",
    "evaluate_agent(env, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_qlearning(env, episodes=1000):\n",
    "    # variables\n",
    "    total_timesteps = 0\n",
    "    scores = []\n",
    "    gamma = 0.7\n",
    "    alpha = 0.5\n",
    "\n",
    "    model = get_model(env)\n",
    "\n",
    "    # TODO: remove this return statement\n",
    "    return model, scores\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # start episode and get initial observation\n",
    "        state = env.reset()\n",
    "        epsilon = get_epsilon(episode)\n",
    "\n",
    "        # reset score and epochs\n",
    "        score = 0\n",
    "        epochs = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # loop through steps\n",
    "        while not done:\n",
    "            # TODO: implement Q-learning\n",
    "\n",
    "            # update score\n",
    "            score += reward\n",
    "\n",
    "            # update epoch\n",
    "            epochs += 1\n",
    "\n",
    "        print(f'Episode {episode} finished, Score: {score}, Epochs: {epochs}, '\n",
    "              f'Epsilon: {epsilon:.2f}')\n",
    "\n",
    "        total_timesteps += epochs\n",
    "        scores.append(score)\n",
    "\n",
    "    # close the environment\n",
    "    env.close()\n",
    "\n",
    "    return model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = GridEnv()\n",
    "episodes = 500\n",
    "model, scores = train_qlearning(env, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model)\n",
    "display(df)\n",
    "\n",
    "#visualize_training_development(episodes, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "num_steps_agent = test_agent(env, model)\n",
    "evaluate_agent(env, num_steps_agent)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}